{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepscratch.dataset.vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-02-23 14:05:05,152:jax._src.xla_bridge:1018: Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepscratch.dataset.base\n",
      "deepscratch.models.base\n",
      "deepscratch.initialisers\n",
      "deepscratch.activations\n",
      "deepscratch.optimisers\n",
      "deepscratch.losses\n",
      "deepscratch.transformations\n",
      "Metal device set to: Apple M3\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "deepscratch.models.vision.cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1740319505.153021 10202701 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n",
      "I0000 00:00:1740319505.167365 10202701 service.cc:145] XLA service 0x16a452700 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1740319505.167375 10202701 service.cc:153]   StreamExecutor device (0): Metal, <undefined>\n",
      "I0000 00:00:1740319505.168410 10202701 mps_client.cc:406] Using Simple allocator.\n",
      "I0000 00:00:1740319505.168442 10202701 mps_client.cc:384] XLA backend will use up to 11452776448 bytes on device 0 for SimpleAllocator.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import deepscratch\n",
    "from deepscratch.typing import PyTree\n",
    "\n",
    "from deepscratch.dataset.vision import ImageNet\n",
    "from deepscratch.dataset.base import DataLoader\n",
    "from deepscratch.models.base import LinearBlock, Sequential\n",
    "from deepscratch.models.vision.cnn import ConvBlock\n",
    "from deepscratch.initialisers import Gaussian, Zeros\n",
    "from deepscratch.activations import ReLU, Softmax, Activation\n",
    "from deepscratch.optimisers import SGD, Adam\n",
    "from deepscratch.losses import CrossEntropy, Accuracy\n",
    "from deepscratch.transformations import Reshape, AvgPool\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeMachine\n",
    "\n",
    "Below we implement a RNN architecture to predict the next 5 words in the sentence in the book *The Time Machine* by HG Wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import random\n",
    "\n",
    "from deepscratch.models.sequence.embeddings import LSA\n",
    "from deepscratch.dataset.base import DataLoader\n",
    "from deepscratch.dataset.sequence import SequenceDataset\n",
    "from deepscratch.models.sequence.tokeniser import WordTokeniser\n",
    "from deepscratch.models.base import Sequential\n",
    "from deepscratch.models.sequence.rnn import RNNEncoder, RNNDecoder\n",
    "from deepscratch.activations import LinearActivation\n",
    "from deepscratch.losses import RMSE\n",
    "from deepscratch.optimisers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../deepscratch/__deepscratchcache__/thetimemachine.pkl\", \"rb\") as f:\n",
    "    embedder = LSA.from_cache(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = WordTokeniser()\n",
    "\n",
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\", \"rt\") as f:\n",
    "    ds = SequenceDataset(f, 10, 10, tokeniser, embedder)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "dl = DataLoader(ds, BATCH_SIZE, shuffle=True, num_workers=16, drop_last=True, iobound=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 10, 50), (256, 10, 50))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_size = decoder_hidden_size = 50\n",
    "\n",
    "ann = Sequential([\n",
    "    RNNEncoder(x.shape[-1], encoder_hidden_size),\n",
    "    RNNDecoder(y.shape[-2], encoder_hidden_size, y.shape[-1], decoder_hidden_size, output_activation=LinearActivation())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32791, 15150)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds), ann.n_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.initialise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 128\tStep: NAN\tLoss: NAN\n"
     ]
    }
   ],
   "source": [
    "ann.train(\n",
    "    dl,\n",
    "    RMSE(),\n",
    "    Adam,\n",
    "    lr=1e-4,\n",
    "    epochs=1,\n",
    "    device=\"METAL\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\", \"rt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "corpus = jnp.unique(tokeniser.tokenise(text))\n",
    "corpus_embeddings = embedder.embed(corpus)\n",
    "\n",
    "def to_words(arr: jnp.array) -> list[str]:\n",
    "    \"\"\"\n",
    "    Map predicted embeddings at each step to NL words\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for t in range(arr.shape[-2]):\n",
    "        euc_dist = ((embedder.embeddings - arr[...,t,:]) ** 2).sum(axis=-1)\n",
    "        i = euc_dist.argmin()\n",
    "        words.append(embedder.idx_to_token[i.item()])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: that they were very badly broken and weather worn several\n",
      "Predicted Passage: unsatisfying probably dawn played calm specimens return specimens return specimens\n"
     ]
    }
   ],
   "source": [
    "# Take one sample\n",
    "i = random.randint(0, len(dl))\n",
    "for _, (x,y) in zip(range(i), dl):\n",
    "    pass\n",
    "\n",
    "x0, y0 = x[jnp.array([0])], y[jnp.array([0])] \n",
    "y_est = ann.forward(x)\n",
    "y0_est = y_est[jnp.array([0])]\n",
    "\n",
    "print(f\"Input: {\" \".join(to_words(x0))}\\nPredicted Passage: {\" \".join(to_words(y0_est))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model picks out some semantic structure but is it by-and-large non-sensical -- reflecting the overparamterisation of the model. The novel is too short compared with the complexity of the english language to provide sufficient training data for our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
