{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "Activation functions are simple, non-linear layers in a neural network that induce a great flexibility in the range of functions that a neural network can approximate, whilst being simple and easily-differentiable, and so enabling the use of backpropogation to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(ABC):\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def forward(x: jnp.array) -> jnp.array:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearActivation(ABC):\n",
    "    @staticmethod\n",
    "    def forward(x: jnp.array) -> jnp.array:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Activation Function\n",
    "The **Sigmoid** activation function maps any real-valued input to a range between \\(0\\) and \\(1\\), making it useful for probability-based outputs in binary classification problems. The function is defined as:  \n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n",
    "where $e^{-x}$ ensures smooth Â§and continuous output. The sigmoid function is differentiable, but it suffers from the **vanishing gradient problem**, making it less suitable for deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return 1 / (1 + jnp.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return jnp.exp(-jnp.logaddexp(0, -x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU (Rectified Linear Unit) Activation Function\n",
    "The **ReLU (Rectified Linear Unit)** activation function is widely used in deep learning due to its simplicity and effectiveness in mitigating vanishing gradient issues. It is defined as:  \n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$  \n",
    "or, in the generalized form with a threshold \\(t\\):  \n",
    "$$\n",
    "\\text{ReLU}(x) = \\begin{cases} \n",
    "x, & x > t \\\\ \n",
    "0, & x \\leq t \n",
    "\\end{cases}\n",
    "$$\n",
    "ReLU is computationally efficient and promotes sparsity in activations, but it can suffer from the **dying ReLU problem**, where neurons become inactive for negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def __init__(self, threshold=0) -> None:\n",
    "        self.threshold = threshold\n",
    "        self.forward = partial(self.forward, threshold=self.threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x, threshold) -> jnp.array:\n",
    "        return jnp.where(x>threshold, x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh (Hyperbolic Tangent) Activation Function \n",
    "The **Tanh** activation function is a scaled version of the sigmoid function, mapping inputs to a range between \\(-1\\) and \\(1\\). The function is given by:  \n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "$$  \n",
    "or equivalently, using the sigmoid function:  \n",
    "$$\n",
    "\\tanh(x) = 2\\sigma(2x) - 1\n",
    "$$  \n",
    "Tanh is zero-centered, making it preferable over sigmoid for training deep networks, though it still suffers from vanishing gradients for large or small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x) -> jnp.array:\n",
    "        return 2/(1 + jnp.exp(-2*x)) - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function \n",
    "The **Softmax** function is used primarily in classification tasks where outputs need to be interpreted as probabilities. It normalizes an input vector into a probability distribution by computing:  \n",
    "$$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}\n",
    "$$  \n",
    "where the subtraction of $\\max(x)$ (log-sum-exp trick) helps prevent numerical instability. Softmax ensures that all output values sum to 1, making it ideal for multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Activation):\n",
    "    def __init__(self, axis=-1):\n",
    "        self.axis = axis\n",
    "        self.forward = partial(self.forward, axis=self.axis)\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x, axis):\n",
    "        \"\"\"\n",
    "        'axis' = axis to compute softmax over\n",
    "        \"\"\"\n",
    "        # Use log-sum-exp trickt to circumvent under/over flow\n",
    "        x_max = jnp.max(x, axis=axis, keepdims=True)\n",
    "        exp_x = jnp.exp(x-x_max)\n",
    "        return  exp_x / exp_x.sum(axis=axis, keepdims=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
