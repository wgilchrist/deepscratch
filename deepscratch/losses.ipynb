{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import jax.numpy as jnp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions map a vector of model predictions and a corresponding vector of ground truthes to a scalar:\n",
    "$$\n",
    "\\mathcal{L}: \\mathbb{R}^N \\times \\mathbb{R}^N \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "\n",
    "The greater the output of the loss function, the worse the model performs. A good loss function should encourage \"high-quality\" responses from the model, perhaps including:\n",
    " - Matching the ground truth in any given instance\n",
    " - Matching the ground truth in a particular, minority class (i.e imbalanced learning)\n",
    " - Mathcing the ground truth for \"critical\" observations (e.g. fraud detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_est: jnp.array, y: jnp.array) -> jnp.array:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "class Accuracy(Loss):\n",
    "\n",
    "    def forward(self, y_est, y):\n",
    "        return (y == y_est).sum() / y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error\n",
    "\n",
    "The **Mean Squared Error (MSE)** is a convex loss function defined as the second moment of the error distribution. Given a set of true labels \\( y \\in \\mathbb{R}^N \\) and estimated predictions \\( \\hat{y} \\in \\mathbb{R}^N \\), the loss is computed as:  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{MSE}}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where \\( N \\) is the number of samples. The gradient of MSE with respect to \\( \\hat{y} \\) is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{MSE}}}{\\partial \\hat{y}_i} = -\\frac{2}{N} (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "which follows from differentiating the squared error term. Since the function is quadratic, it ensures convexity, making it suitable for gradient-based optimization. However, due to the squared term, MSE assigns higher penalty to large deviations, increasing its sensitivity to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "    @staticmethod\n",
    "    def forward(y_est, y):\n",
    "        err = y - y_est\n",
    "        loss = (err**2).sum() / err.size\n",
    "        return loss\n",
    "    \n",
    "class RMSE(Loss):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(y_est, y):\n",
    "        err = y - y_est\n",
    "        loss = (err**2).sum() / err.size\n",
    "        return jnp.sqrt(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross-Entropy Loss**  \n",
    "The **Cross-Entropy Loss** is derived from the Kullback-Leibler (KL) divergence, measuring the difference between two probability distributions. Given true class labels $ y \\in \\{0,1\\}^N $ and predicted probabilities $ \\hat{y} \\in [0,1]^N $, the **binary cross-entropy** loss is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{BCE}}(y, \\hat{y}) = - \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "which is derived from the likelihood function of a Bernoulli distribution under maximum likelihood estimation (MLE). For multi-class classification with $ C $ possible classes, where $ y $ is one-hot encoded, the **categorical cross-entropy** extends to:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CCE}}(y, \\hat{y}) = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "where $y_{ij} $ is the true probability (1 for the correct class, 0 otherwise), and $ \\hat{y}_{ij} $ is the softmax-normalized predicted probability for class $ j $:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{ij} = \\frac{e^{z_{ij}}}{\\sum_{k=1}^{C} e^{z_{ik}}}\n",
    "$$\n",
    "\n",
    "The gradient with respect to $ z_{ij} $ simplifies to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{CCE}}}{\\partial z_{ij}} = \\hat{y}_{ij} - y_{ij}\n",
    "$$\n",
    "\n",
    "which results in a stable gradient for optimizing classification models. Unlike MSE, cross-entropy loss is well-suited for probabilistic interpretations and avoids issues of slow convergence when used with softmax activation in multi-class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Loss):\n",
    "    def __init__(self, eps=1e-8, axis=-1):\n",
    "        self.eps = eps\n",
    "        self.axis = axis\n",
    "        self.forward = partial(self.forward, eps = self.eps, axis = self.axis)\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(y_est, y, eps, axis):\n",
    "        y_est = jnp.clip(y_est, eps, 1 - eps)\n",
    "        loss = -jnp.mean(jnp.sum(y * jnp.log(y_est), axis=axis))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Huber Loss**  \n",
    "Huber loss is a **robust loss function** that is less sensitive to outliers than Mean Squared Error (MSE) while maintaining smooth gradients. It combines **L1 loss** (absolute error) and **L2 loss** (squared error) based on a threshold \\( \\delta \\). It is formally defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{Huber}}(y, \\hat{y}) =\n",
    "\\begin{cases} \n",
    "\\frac{1}{2} (y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "\\delta \\left( |y - \\hat{y}| - \\frac{1}{2} \\delta \\right), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $ \\delta $ is a hyperparameter controlling the transition point between quadratic and linear behavior. The gradient with respect to \\( \\hat{y} \\) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{Huber}}}{\\partial \\hat{y}} =\n",
    "\\begin{cases} \n",
    "-(y - \\hat{y}), & |y - \\hat{y}| \\leq \\delta \\\\\n",
    "-\\delta \\cdot \\text{sign}(y - \\hat{y}), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Huber loss behaves like **MSE** for small residuals and like **MAE (Mean Absolute Error)** for large residuals, providing robustness against outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Huber(Loss):\n",
    "    def __init__(self, delta=1.0):\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, y_est, y):\n",
    "        err = y - y_est\n",
    "        abs_err = jnp.abs(err)\n",
    "        quadratic = 0.5 * (err**2)\n",
    "        linear = self.delta * (abs_err - 0.5 * self.delta)\n",
    "        loss = jnp.where(abs_err <= self.delta, quadratic, linear)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hinge Loss (SVM Loss)**  \n",
    "Hinge loss is primarily used in **Support Vector Machines (SVMs)** for classification. Given a set of labels $ y \\in \\{-1, 1\\} $ and predictions $ f(x) $, the **hinge loss** is defined as:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{Hinge}}(y, f(x)) = \\sum_{i=1}^{N} \\max(0, 1 - y_i f(x_i))\n",
    "$$\n",
    "\n",
    "This loss function enforces a margin of at least 1 for correct classifications. If $ y_i f(x_i) \\geq 1 $, the loss is zero (correct classification with sufficient margin), otherwise, it penalizes the incorrect or weakly correct predictions.\n",
    "\n",
    "The gradient with respect to $ f(x) $ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{Hinge}}}{\\partial f(x_i)} =\n",
    "\\begin{cases} \n",
    "- y_i, & y_i f(x_i) < 1 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since hinge loss is **non-differentiable** at $ y_i f(x_i) = 1 $, subgradient methods are used in optimization.\n",
    "\n",
    "For multi-class classification, hinge loss is extended as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{Multi-Hinge}}(y, f(x)) = \\sum_{i=1}^{N} \\sum_{j \\neq y_i} \\max(0, f_j(x_i) - f_{y_i}(x_i) + 1)\n",
    "$$\n",
    "\n",
    "where $ f_j(x_i) $ is the raw score for class $ j $, and $ f_{y_i}(x_i) $ is the score for the correct class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hinge(Loss):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, y_est, y):\n",
    "        loss = jnp.max(0, 1 - y * y_est)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kullback-Leibler (KL) Divergence Loss**  \n",
    "KL divergence is a measure of how one probability distribution $ P(y) $ differs from another distribution $ Q(y) $ (e.g., a model's predicted distribution). It is given by:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P || Q) = \\sum_{i=1}^{N} P(y_i) \\log \\frac{P(y_i)}{Q(y_i)}\n",
    "$$\n",
    "\n",
    "In the context of deep learning, $ P(y) $ represents the true probability distribution (e.g., one-hot encoded ground truth labels), and $ Q(y) $ represents the predicted probability distribution (e.g., softmax outputs). The loss function is thus:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{KL}} = \\sum_{i=1}^{N} y_i \\log y_i - y_i \\log \\hat{y}_i\n",
    "$$\n",
    "\n",
    "Since the first term is independent of the model, KL divergence loss is often computed as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{KL}} = -\\sum_{i=1}^{N} y_i \\log \\hat{y}_i\n",
    "$$\n",
    "\n",
    "which is equivalent to **categorical cross-entropy** when $ y_i $ is one-hot encoded.\n",
    "\n",
    "The gradient with respect to $ \\hat{y} $ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{KL}}}{\\partial \\hat{y}_i} = - \\frac{y_i}{\\hat{y}_i}\n",
    "$$\n",
    "\n",
    "making it suitable for classification tasks, particularly in **variational inference** and **reinforcement learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergence(Loss):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, y_est, y):\n",
    "        loss = (y * (jnp.log(y) - jnp.log(y_est))).sum(axis=-1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example\n",
    "MSE heavily penalizes large errors, making it highly sensitive to outliers (**2.107 → 8.013**), while Huber grows more slowly (**0.807 → 2.233**), making it more robust. Hinge and Cross-Entropy effectively penalize misclassifications, with Cross-Entropy rising sharply (**0.105 → 2.072**) when confident predictions are incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%ignore` not found.\n"
     ]
    }
   ],
   "source": [
    "losses = {\n",
    "    \"MSE\": MSE(),\n",
    "    \"Huber (δ=1)\": Huber(delta=1.0),\n",
    "    \"Hinge\": Hinge(),\n",
    "    \"KL Divergence\": KLDivergence(),\n",
    "    \"Cross Entropy\": CrossEntropy()\n",
    "}\n",
    "\n",
    "# True labels (regression and classification cases)\n",
    "y_reg = jnp.array([2.0, -1.0, 3.0])  # Regression (MSE)\n",
    "y_cls = jnp.array([1, -1, 1])        # Classification (Hinge)\n",
    "y_prob = jnp.array([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0]])  # OHE (KL, Cross Entropy)\n",
    "\n",
    "# Different prediction scenarios\n",
    "predictions = {\n",
    "    \"Good Predictions\": jnp.array([[0.9, 0.1], [0.1, 0.9], [0.9, 0.1]]),  # Correct + low noise\n",
    "    \"Noisy Predictions\": jnp.array([[0.7, 0.3], [0.3, 0.7], [0.6, 0.4]]),  # Noisy\n",
    "    \"Outlier Predictions\": jnp.array([[0.1, 0.9], [0.9, 0.1], [0.2, 0.8]])  # Bad outliers\n",
    "}\n",
    "\n",
    "# Evaluate losses\n",
    "for loss_name, loss_fn in losses.items():\n",
    "    header = f\"\\n---  {loss_name} Loss Results:  ---\"\n",
    "    print(header)\n",
    "    for scenario, y_est in predictions.items():\n",
    "        if loss_name == \"Hinge\":\n",
    "            result = loss_fn(y_est[:, 0] * 2 - 1, y_cls)\n",
    "        elif loss_name in [\"KL Divergence\", \"Cross Entropy\"]:\n",
    "            y_est = jnp.clip(y_est, 1e-7, 1.0)  # Catch log(0)\n",
    "            result = loss_fn(y_est, y_prob)\n",
    "        else:\n",
    "            result = loss_fn(y_est[:, 0] * 2 - 1, y_reg)\n",
    "        print(f\"{scenario}: {result:.3f}\")\n",
    "    print(\"-\"*len(header))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
