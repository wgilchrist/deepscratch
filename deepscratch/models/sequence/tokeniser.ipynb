{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer, FlaxAutoModel\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def tokenise(self, text: str) -> list[str]:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokeniser(Tokeniser):\n",
    "    def __init__(self, drop_punctuation: bool = True, to_lower: bool = True):\n",
    "\n",
    "        self.drop_punctuation = drop_punctuation\n",
    "        self.to_lower = to_lower\n",
    "        if self.drop_punctuation:\n",
    "            self.pattern = re.compile(r'\\b\\w+\\b')\n",
    "        else:\n",
    "            self.pattern = None\n",
    "\n",
    "        self.token_to_idx = {}\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return list(self.token_to_idx.keys())\n",
    "    \n",
    "    @property\n",
    "    def idx_to_token(self):\n",
    "        return {v:k for k,v in self.token_to_idx.items()}\n",
    "\n",
    "    def tokenise(self, text: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Map a string to a list of string tokens\n",
    "        \"\"\"\n",
    "        if self.to_lower:\n",
    "            text = text.lower()\n",
    "        \n",
    "        if self.drop_punctuation:\n",
    "            str_tokens = self.pattern.findall(text)\n",
    "        else:\n",
    "            str_tokens = text.split()\n",
    "\n",
    "        int_tokens = []\n",
    "        for t in str_tokens:\n",
    "            if t not in self.token_to_idx:\n",
    "                self.token_to_idx[t] = len(self.token_to_idx)\n",
    "\n",
    "            int_tokens.append(self.token_to_idx[t])\n",
    "\n",
    "        return jnp.array(int_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:\t i\n",
      "01:\t introduction\n",
      "02:\t the\n",
      "03:\t time\n",
      "04:\t traveller\n",
      "05:\t for\n",
      "06:\t so\n",
      "07:\t it\n",
      "08:\t will\n",
      "09:\t be\n",
      "10:\t convenient\n",
      "11:\t to\n",
      "12:\t speak\n",
      "13:\t of\n",
      "14:\t him\n",
      "15:\t was\n",
      "16:\t expounding\n",
      "17:\t a\n",
      "18:\t recondite\n",
      "19:\t matter\n",
      "11:\t to\n",
      "20:\t us\n",
      "21:\t his\n",
      "22:\t pale\n",
      "23:\t grey\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\", \"rt\") as f:\n",
    "    tokeniser = WordTokeniser(f)\n",
    "\n",
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\", \"rt\") as f:\n",
    "    idxs = []\n",
    "    for _, line in zip(range(20), f.readlines()):\n",
    "        idxs.extend(tokeniser.tokenise(line))\n",
    "    tokens = [tokeniser.idx_to_token[i.item()] for i in idxs]\n",
    "\n",
    "    cap = 25\n",
    "    for t, i, _ in zip(tokens, idxs, range(cap)):\n",
    "        print(f\"{i:02}:\\t {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class HFTokeniser:\n",
    "    def __init__(self, model_name, max_length=512, truncation=True, add_special_tokens=True, *args, **kwargs):\n",
    "        self._model = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.truncation = truncation\n",
    "        self.add_special_tokens = add_special_tokens\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return list(set(self._model.vocab.keys()))\n",
    "    \n",
    "    @property\n",
    "    def token_to_idx(self):\n",
    "        return self._model.vocab if self._model.vocab is not None else {}\n",
    "\n",
    "    @property\n",
    "    def idx_to_token(self):\n",
    "        return {v:k for k,v in self.token_to_idx.items()}\n",
    "\n",
    "    def tokenise(self, text: str) -> list[str]:\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return jnp.array([])\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        for i, word in enumerate(words):\n",
    "            #print(f\"Word {i:03d}/{len(words)}\", end=\"\\r\")\n",
    "            # Temporarily tokenize the current chunk plus the next word to see its length\n",
    "            temp_tokens = self._model(\" \".join(current_chunk + [word]), add_special_tokens=self.add_special_tokens, truncation=self.truncation, *self.args, **self.kwargs)['input_ids']\n",
    "            if len(temp_tokens) < 512:\n",
    "                current_chunk.append(word)\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [word]\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        # Tokenize each chunk without truncation now, as each should be within the limit\n",
    "        tokens_list = []\n",
    "        for chunk in chunks:\n",
    "            tokens = self._model(chunk, return_tensors='jax', truncation=self.truncation, add_special_tokens=self.add_special_tokens, *self.args, **self.kwargs)['input_ids'][0]\n",
    "            tokens_list.append(tokens)\n",
    "        \n",
    "        # Concatenate the tokens along the sequence dimension\n",
    "        return jnp.concatenate(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 101 1045 1012 ... 2158 1012  102]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\", \"rt\") as f:\n",
    "    tokeniser = HFTokeniser('bert-base-uncased')\n",
    "    print(tokeniser.tokenise(f.read()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
