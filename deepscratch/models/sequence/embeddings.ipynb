{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "from jax.experimental import sparse as jaxsparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import io \n",
    "\n",
    "from scipy.sparse import coo_array, linalg\n",
    "\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "from deepscratch.models.sequence.tokeniser import Tokeniser, WordTokeniser, HFTokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\", \"rt\") as f:\n",
    "    tokeniser = HFTokeniser('bert-base-uncased')\n",
    "    tokeniser.tokenise(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_words(embed, target_word, n):\n",
    "    \"\"\"\n",
    "    Return the n closest words to target_words with reference to embed.\n",
    "\n",
    "    Measured by cosine similarity.\n",
    "    \"\"\"\n",
    "    # get target embedding\n",
    "    target_embedding = embed.word_embeddings[\n",
    "        embed.corpus.word2idx[target_word]\n",
    "    ]\n",
    "\n",
    "    cosine_sim = (\n",
    "        (embed.word_embeddings[1:] * target_embedding).sum(axis=1)\n",
    "        / jnp.sqrt(\n",
    "            (embed.word_embeddings[1:] ** 2).sum(axis=1)\n",
    "            * (target_embedding ** 2).sum()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    euc_dist = (\n",
    "        ((embed.word_embeddings[1:] - target_embedding) ** 2).sum(axis=1)\n",
    "    )\n",
    "    min_idx = jnp.argpartition(cosine_sim, -n)[-n:] + 1 # add 1 as we removed first row of nans\n",
    "\n",
    "    # print to stdout\n",
    "    print(f\"Target token: {target_word}\")\n",
    "    print(f\"Nearest words (no order): {[embed.corpus.word2idx.reversed[i] for i in min_idx.tolist()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embed = jax.vmap(self.embed)\n",
    "   \n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def embed(idx: int) -> jnp.array:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHE(Embedder):\n",
    "\n",
    "    def __init__(self, f, tokeniser: Tokeniser):\n",
    "        \n",
    "        self.tokeniser = tokeniser\n",
    "\n",
    "        f.seek(0)\n",
    "        self.tokens = self.tokeniser.tokenise(f.read())\n",
    "        self.embed = partial(self.embed, N=len(self.tokeniser.idx_to_token))\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def embed(idx: int, N: jnp.array):\n",
    "        return jnp.zeros(N).at[idx].set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with io.StringIO(\"The quick brown fox jumped over the lazy dog.\") as f: \n",
    "    embedder = OHE(f, tokeniser)\n",
    "\n",
    "token = \"fox\"\n",
    "idx = embedder.tokeniser.token_to_idx[token]\n",
    "embedder.embed(jnp.array([idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSA(Embedder):\n",
    "    def __init__(self, embeddings, tokeniser_hash, weighting, token_to_idx, idx_to_token=None):\n",
    "        self.embeddings = embeddings\n",
    "        self.tokeniser_hash = tokeniser_hash\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.weighting = weighting\n",
    "\n",
    "        if idx_to_token is not None:\n",
    "            self.idx_to_token = idx_to_token\n",
    "        else:\n",
    "            self.idx_to_token = {v: k for k, v in token_to_idx.items()}\n",
    "\n",
    "        self.embed = partial(\n",
    "            self.embed,\n",
    "            embeddings=self.embeddings\n",
    "        )\n",
    "        super().__init__()\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, f, tokeniser, window_len=20, k=100, weighting=\"ppmi\"):\n",
    "        f.seek(0)\n",
    "        tokens = tokeniser.tokenise(f.read())\n",
    "        f.close()\n",
    "\n",
    "        embeddings = cls._compute_word_embeddings(tokens, tokeniser, weighting, window_len, k)\n",
    "        return cls(embeddings, hash(tokeniser), weighting, tokeniser.token_to_idx, tokeniser.idx_to_token)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_cache(cls, f):\n",
    "        tokeniser_hash, embeddings, token_to_idx, weighting = pickle.load(f)\n",
    "        return cls(embeddings, tokeniser_hash, weighting, token_to_idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_word_embeddings(tokens, tokeniser, weighting, window_len, k):\n",
    "        n_words = len(tokeniser.token_to_idx)\n",
    "        n_windows = len(tokens) - window_len + 1\n",
    "        tdfm_np = np.zeros((n_words, n_windows), dtype=np.float32)\n",
    "        print(tdfm_np.shape)\n",
    "\n",
    "        for window in range(n_windows):\n",
    "            for token in tokens[window:window+window_len]:\n",
    "                tdfm_np[token, window] += 1\n",
    "\n",
    "        if weighting in {\"pmi\", \"ppmi\"}:\n",
    "            word_freqs = tdfm_np.sum(axis=1, keepdims=True)\n",
    "            context_freqs = tdfm_np.sum(axis=0, keepdims=True)\n",
    "            total_count = tdfm_np.sum()\n",
    "            p_wc = tdfm_np / total_count\n",
    "            p_w = word_freqs / total_count\n",
    "            p_c = context_freqs / total_count\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                pmi = np.log2(p_wc / (p_w @ p_c))\n",
    "                pmi[np.isnan(pmi)] = 0  # Replace NaNs with 0\n",
    "                pmi[np.isinf(pmi)] = 0  # Replace infinities with 0\n",
    "\n",
    "            if weighting == \"ppmi\":\n",
    "                pmi = np.maximum(pmi, 0)\n",
    "\n",
    "            tdfm_np = pmi\n",
    "        \n",
    "        U, S, _ = np.linalg.svd(tdfm_np, full_matrices=False)\n",
    "        U_k = U[:, :k]\n",
    "        S_k = S[:k]\n",
    "        \n",
    "        embeddings = U_k * S_k[None, :]  # shape (N, k)\n",
    "        embeddings = jnp.array(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "    def cache(self, f):\n",
    "        pickle.dump((self.tokeniser_hash, self.embeddings, self.token_to_idx, self.weighting), f)\n",
    "\n",
    "    @staticmethod\n",
    "    def embed(idx: int, embeddings: jnp.array):\n",
    "        \"\"\"\n",
    "        Given a token index, returns its latent semantic vector.\n",
    "        \"\"\"\n",
    "        return embeddings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = WordTokeniser()\n",
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\") as f: \n",
    "    tokens = tokeniser.tokenise(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4609, 32781)\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\") as f: \n",
    "    embedder = LSA.from_file(f, tokeniser, window_len=30, k=50, weighting=\"ppmi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_words(\n",
    "    token: str,\n",
    "    embedder: LSA,\n",
    "    n_words: int = 10\n",
    "):\n",
    "    embeddings = embedder.embeddings\n",
    "    idx = embedder.token_to_idx[token]\n",
    "    word_embedding = embedder.forward(jnp.array([idx]))\n",
    "    \n",
    "\n",
    "    distances = ((embeddings - word_embedding) ** 2).sum(axis=1)\n",
    "    nearest_idxs = jnp.argpartition(distances, n_words)[:10]\n",
    "    nearest_words = [embedder.idx_to_token[i.item()] for i in nearest_idxs]\n",
    "\n",
    "    return nearest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark',\n",
       " 'struck',\n",
       " 'daylight',\n",
       " 'drove',\n",
       " 'flinging',\n",
       " 'thinking',\n",
       " 'confidence',\n",
       " 'moon',\n",
       " 'box',\n",
       " 'near']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = \"dark\"\n",
    "nearest_words(token, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = HFTokeniser('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FlaxBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: {('pooler', 'dense', 'kernel'), ('pooler', 'dense', 'bias')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class HFEmbedder:\n",
    "\n",
    "    def __init__(self, model_name, *args, **kwargs):\n",
    "        self._model = FlaxAutoModel.from_pretrained(model_name, *args, **kwargs)\n",
    "    \n",
    "    def embed(self, tokens):\n",
    "        return self._model(tokens.reshape(-1,1)).last_hidden_state[:,0,:]\n",
    "\n",
    "hf_embedder = HFEmbedder('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\", \"rt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "corpus = jnp.unique(tokeniser.tokenise(text))\n",
    "corpus_embeddings = hf_embedder.embed(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 nearest tokens to 'sun':\n",
      " - sun\n",
      " - ##ng\n",
      " - dream\n",
      " - remark\n",
      " - puzzle\n",
      " - ##ath\n",
      " - cad\n",
      " - weed\n",
      " - ##ffin\n",
      " - know\n"
     ]
    }
   ],
   "source": [
    "TOKEN = \"sun\"\n",
    "n_words = 10\n",
    "token_id = tokeniser.tokenise(TOKEN)[1:-1]\n",
    "assert token_id.shape[0] == 1, f\"{token_id}\"\n",
    "word_embedding = hf_embedder.embed(token_id)\n",
    "\n",
    "distances = (\n",
    "    ((corpus_embeddings - word_embedding) ** 2).sum(axis=1) \n",
    "    / (jnp.linalg.norm(corpus_embeddings, axis=1) * (jnp.linalg.norm(word_embedding)))\n",
    ")\n",
    "nearest_idxs = jnp.argpartition(distances, n_words)[:n_words]\n",
    "nearest_tokens = corpus[nearest_idxs]\n",
    "nearest_words = [tokeniser.idx_to_token[i.item()] for i in nearest_tokens]\n",
    "\n",
    "print(f\"{n_words} nearest tokens to '{TOKEN}':\\n - \"+\"\\n - \".join(nearest_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
