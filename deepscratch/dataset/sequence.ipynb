{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepscratch.dataset.base\n",
      "deepscratch.models.sequence.tokeniser\n",
      "deepscratch.models.sequence.embeddings\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial \n",
    "\n",
    "from deepscratch.dataset.base import Dataset\n",
    "from deepscratch.models.sequence.tokeniser import WordTokeniser, Tokeniser\n",
    "from deepscratch.models.sequence.embeddings import Embedder, OHE, LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential datasets are iterables over Corpus objects.\n",
    " - Seq2Vec yields the next thing in the corpus along with the next y value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-02-19 12:41:21,189:jax._src.xla_bridge:1018: Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M3\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1739968881.190097 8712519 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n",
      "I0000 00:00:1739968881.207415 8712519 service.cc:145] XLA service 0x104255280 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739968881.207427 8712519 service.cc:153]   StreamExecutor device (0): Metal, <undefined>\n",
      "I0000 00:00:1739968881.208594 8712519 mps_client.cc:406] Using Simple allocator.\n",
      "I0000 00:00:1739968881.208604 8712519 mps_client.cc:384] XLA backend will use up to 11452776448 bytes on device 0 for SimpleAllocator.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../__deepscratchcache__/thetimemachine.pkl\", \"rb\") as f:\n",
    "    embedder = LSA.from_cache(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        f,\n",
    "        input_len,\n",
    "        output_len,\n",
    "        tokeniser: Tokeniser,\n",
    "        embedder: Embedder,\n",
    "    ):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.tokeniser = tokeniser\n",
    "        self.embedder = embedder\n",
    "\n",
    "        # Read the full text and tokenise\n",
    "        f.seek(0)\n",
    "        self.tokens = jnp.array(tokeniser.tokenise(f.read()))\n",
    "        f.close()\n",
    "        \n",
    "        # Bind params\n",
    "        self._jitgetitem = partial(\n",
    "            self._jitgetitem,\n",
    "            tokens=self.tokens,\n",
    "            input_len=self.input_len,\n",
    "            output_len=self.output_len,\n",
    "            embedder_func=self.embedder.embed\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of possible sequences (windows) available.\n",
    "        For index i, __getitem__(i) will return tokens[i : i + window_len].\n",
    "        \"\"\"\n",
    "        return len(self.tokens) - self.input_len - self.output_len + 1\n",
    "    \n",
    "    def __getitem__(self, index: int) -> dict[str, jnp.array]:\n",
    "        return self._jitgetitem(index)\n",
    "\n",
    "    @staticmethod\n",
    "    def _jitgetitem(\n",
    "        index: int,\n",
    "        tokens,\n",
    "        input_len,\n",
    "        output_len,\n",
    "        embedder_func\n",
    "    ) -> jnp.ndarray:\n",
    "        # Extract the token window\n",
    "        window = jax.lax.dynamic_slice(\n",
    "            tokens, (index,), (input_len + output_len,)\n",
    "        )\n",
    "        \n",
    "        # Embed the tokens on demand (this may be memory heavy, but only for a small window)\n",
    "        embedded_window = embedder_func(window)\n",
    "        x, y = embedded_window[...,:input_len,:], embedded_window[...,input_len:,:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[-7.0361847e+02,  3.3582416e+02,  1.6287428e+02],\n",
       "        [-1.1196708e-02, -9.6939774e-03, -2.0985298e-02],\n",
       "        [-1.3034204e+03, -3.2633771e+02,  1.4264659e+02],\n",
       "        [-1.0618215e+02, -3.3759235e+01,  6.7791662e+00],\n",
       "        [-3.2326031e+01, -2.8942438e+01,  8.6696997e+00]], dtype=float32),\n",
       " Array([[-2.2718998e+01,  3.7305080e+01, -4.2888706e+01,  1.0025843e+02,\n",
       "         -1.6993481e+01, -1.6425310e+01,  4.7479172e+01],\n",
       "        [ 1.0182316e-02,  6.3046985e-03,  1.1931198e-02, -5.4324535e-03,\n",
       "          3.7624363e-03, -1.0620670e-02,  8.6728754e-03],\n",
       "        [ 4.0849900e+00,  5.5485703e+01, -2.1984842e+01, -1.8106064e+01,\n",
       "         -1.2444858e+01,  1.5683472e+00, -4.9172010e+00],\n",
       "        [ 4.8439548e+01, -7.0512054e+01, -6.3593425e-02,  6.6587166e+01,\n",
       "          1.9610695e+00, -2.0335042e+00, -1.3289231e+01],\n",
       "        [ 4.0634182e+01, -3.2517994e+01, -3.6052942e+00,  2.7541046e+01,\n",
       "         -5.4291263e+00, -8.5053606e+00,  1.7380468e+00]], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser = WordTokeniser()\n",
    "with open(\"/Users/willgilchrist/dev/deeplearning/data/books/timemachine.txt\", \"rt\") as f:\n",
    "    ds = SequenceDataset(f, 3, 2, tokeniser, embedder, window_len=5)\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepscratch.dataset.base import DataLoader\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "dl = DataLoader(ds, BATCH_SIZE, shuffle=True, num_workers=16, drop_last=True, iobound=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2VecDataset:\n",
    "    def __init__(self, corpus, y, embed, batch_size):\n",
    "        self.corpus = corpus\n",
    "        self.y = y\n",
    "        self.i = 0\n",
    "        self.embed = embed\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        i = 0\n",
    "        for _ in self.corpus:\n",
    "            i+=1\n",
    "        self.corpus.reset()\n",
    "        return i\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            batch_idx = next(self.corpus)\n",
    "        except StopIteration:\n",
    "            self.corpus.reset()\n",
    "            self.i = 0\n",
    "            batch_idx = next(self.corpus)\n",
    "\n",
    "        batch = self.embed(batch_idx)\n",
    "        y = self.y[self.i:self.i + len(batch)]\n",
    "        self.i += len(batch)\n",
    "\n",
    "        return {\"x\":batch, \"y\":y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq datasets iterate over the corpus and split that into the input token and output token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset:\n",
    "    def __init__(self, iterator, input_len, output_len, embed, batch_size, target_memory_usage=100):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.batch_size = batch_size\n",
    "        self.iterator = iterator\n",
    "        self.embed = embed\n",
    "\n",
    "    def __len__(self):\n",
    "        i = 0\n",
    "        for _ in self.corpus:\n",
    "            i+=1\n",
    "        self.iterator.reset()\n",
    "        return i\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=1)\n",
    "    def split_batch(batch, cutoff):\n",
    "        return batch[:,:cutoff], batch[:, cutoff:]\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            next_batch_idx = next(self.iterator)\n",
    "        except StopIteration:\n",
    "            self.iterator.reset()\n",
    "            next_batch_idx = next(self.iterator)\n",
    "\n",
    "        next_batch = self.embed(next_batch_idx)            \n",
    "        x, y = self.split_batch(next_batch, self.input_len)\n",
    "\n",
    "        return {\"x\":x, \"y\":y}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
