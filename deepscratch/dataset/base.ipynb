{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Iterator, Tuple, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import deepscratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package implements datasets as follows: they have a length and support indexing such that the user can interrogate the data. They are also one-time iterable, meaning they yield observations sequentially until a StopIteration is raised, at which point the dataset is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __len__(self) -> int:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    A PyTorch-like DataLoader for JAX with multi-threaded batching.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        drop_last: bool = False,\n",
    "        num_workers: int = 1,\n",
    "        seed: Optional[int] = None,\n",
    "        iobound: bool = True\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed if seed is not None else np.random.randint(0, int(1e6))\n",
    "        self.indices = jnp.arange(len(dataset))\n",
    "        self.rng_key = random.PRNGKey(self.seed)\n",
    "        self.iobound = iobound\n",
    "\n",
    "    def _fetch_item(self, index: int) -> Any:\n",
    "        return self.dataset[int(index)]\n",
    "\n",
    "    def __iter__(self) -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
    "        if self.shuffle:\n",
    "            self.rng_key, subkey = random.split(self.rng_key)\n",
    "            self.indices = random.permutation(subkey, self.indices)\n",
    "\n",
    "        if self.iobound:\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "                for image, label in executor.map(self._fetch_item, self.indices):\n",
    "                    batch_images.append(image)\n",
    "                    batch_labels.append(label)\n",
    "                    if len(batch_images) == self.batch_size:\n",
    "                        yield np.stack(batch_images), np.stack(batch_labels)\n",
    "                        batch_images = []\n",
    "                        batch_labels = []\n",
    "\n",
    "            if batch_images and not self.drop_last:\n",
    "                yield np.stack(batch_images), np.stack(batch_labels)\n",
    "        \n",
    "        else:\n",
    "            batched_getitem = jax.vmap(self.dataset._jitgetitem)\n",
    "            for i in range(len(self)):\n",
    "                batch_indices = self.indices[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                batch = batched_getitem(batch_indices)\n",
    "                yield batch\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        size = len(self.dataset) // self.batch_size\n",
    "        if not self.drop_last and len(self.dataset) % self.batch_size != 0:\n",
    "            size += 1\n",
    "        return size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
