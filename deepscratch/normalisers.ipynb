{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Techniques in Deep Learning  \n",
    "\n",
    "Normalization techniques are used in deep learning to stabilize training, speed up convergence, and improve generalization. Below are some common examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization  \n",
    "\n",
    "Batch Normalization (BN) normalizes the input across the **batch dimension**. That is, for each feature (channel), it computes the mean and standard deviation **over the batch** and normalizes accordingly.  \n",
    "\n",
    "For an input tensor $ x $ with shape $ (N, C, ...) $ where $ N $ is the batch dimension and $ C $ represents different features (e.g., channels in an image), BN computes the statistics as:  \n",
    "\n",
    "$$\n",
    "\\mu_B = \\frac{1}{N} \\sum_{i=1}^{N} x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_B^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu_B)^2\n",
    "$$\n",
    "\n",
    "Each input is then normalized as:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sigma_B}\n",
    "$$\n",
    "\n",
    "where $ \\sigma_B $ is the standard deviation:\n",
    "\n",
    "$$\n",
    "\\sigma_B = \\sqrt{\\sigma_B^2 + \\epsilon}\n",
    "$$\n",
    "\n",
    "Here, $ \\epsilon $ is a small constant added for numerical stability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "\n",
    "    def __init__(self, batch_dim=0):\n",
    "        self.batch_dim = batch_dim\n",
    "        self.forward = partial(self.forward, batch_dim=batch_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x, batch_dim):\n",
    "        return (x - x.mean(batch_dim, keepdims=True)) / x.std(batch_dim, keepdims=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization  \n",
    "\n",
    "Layer Normalization (LN) normalizes each input independently, computing the statistics **within each observation** across the feature dimensions. Unlike BN, LN does not depend on the batch size and is useful for tasks where batch statistics are unstable, such as reinforcement learning or recurrent neural networks.  \n",
    "\n",
    "For an input tensor $ x $ of shape $ (N, C, ...) $, LN computes the mean and variance **over the feature dimensions** (not the batch dimension):\n",
    "\n",
    "$$\n",
    "\\mu_L = \\frac{1}{C} \\sum_{j=1}^{C} x_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_L^2 = \\frac{1}{C} \\sum_{j=1}^{C} (x_j - \\mu_L)^2\n",
    "$$\n",
    "\n",
    "Each feature is normalized as:\n",
    "\n",
    "$$\n",
    "\\hat{x}_j = \\frac{x_j - \\mu_L}{\\sigma_L}\n",
    "$$\n",
    "\n",
    "where $ \\sigma_L $ is the standard deviation:\n",
    "\n",
    "$$\n",
    "\\sigma_L = \\sqrt{\\sigma_L^2 + \\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def layer_norm(x, batch_dim = 0):\n",
    "    \"\"\"\n",
    "    Normalise each observation to zero mean and unit variance, computing\n",
    "    statistics within observations, over channels.\n",
    "    \"\"\" \n",
    "    over_dims = tuple(i for i in range(x.ndim) if i != batch_dim)\n",
    "    return (x - x.mean(over_dims, keepdims=True)) / x.std(over_dims, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Normalization  \n",
    "\n",
    "**Instance Normalization (IN)** is similar to Layer Normalization, but it computes statistics **separately for each feature map and each sample**. It is particularly useful for style transfer and generative models.  \n",
    "\n",
    "For an input $$ x $$ with shape $$ (N, C, H, W) $$, IN computes the mean and variance for each instance **per channel**:\n",
    "\n",
    "$$\n",
    "\\mu_I = \\frac{1}{H W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{h, w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_I^2 = \\frac{1}{H W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{h, w} - \\mu_I)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{x - \\mu_I}{\\sigma_I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def instance_norm(x, feature_dim=1):\n",
    "    \"\"\"\n",
    "    Normalizes each feature map independently for each sample.\n",
    "    Used in style transfer and generative models.\n",
    "    \"\"\"\n",
    "    spatial_dims = tuple(i for i in range(x.ndim) if i != feature_dim and i != 0)\n",
    "    return (x - x.mean(spatial_dims, keepdims=True)) / x.std(spatial_dims, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Normalization  \n",
    "\n",
    "**Group Normalization (GN)** divides the channels into groups and normalizes **within each group**. Unlike BatchNorm, it does not depend on batch size, making it useful for small-batch training.  \n",
    "\n",
    "For an input $ x $ with shape $ (N, C, H, W) $, GN splits $ C $ into $ G $ groups. Within each group:\n",
    "\n",
    "$$\n",
    "\\mu_G = \\frac{1}{|G|} \\sum_{c \\in G} x_c\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_G^2 = \\frac{1}{|G|} \\sum_{c \\in G} (x_c - \\mu_G)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{x - \\mu_G}{\\sigma_G}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def group_norm(x, num_groups=8, feature_dim=1):\n",
    "    \"\"\"\n",
    "    Normalizes within groups of feature maps.\n",
    "    Suitable for small batch training.\n",
    "    \"\"\"\n",
    "    N, C = x.shape[:2]\n",
    "    G = min(num_groups, C)  # Ensure we don't have more groups than channels\n",
    "    x = x.reshape(N, G, C // G, *x.shape[2:])  # Reshape to groups\n",
    "    mean = x.mean(axis=(2, 3, 4), keepdims=True)\n",
    "    std = x.std(axis=(2, 3, 4), keepdims=True)\n",
    "    x_norm = (x - mean) / (std + 1e-5)\n",
    "    return x_norm.reshape(N, C, *x.shape[3:])  # Reshape back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Scaling (Weight Standardization)  \n",
    "\n",
    "Instead of normalizing activations, **Weight Standardization** normalizes the weights of the convolutional layers. This is useful for stabilizing training in architectures like ResNets.  \n",
    "\n",
    "For a weight matrix $ W $:\n",
    "\n",
    "$$\n",
    "\\hat{W} = \\frac{W - \\mu_W}{\\sigma_W}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\mu_W = \\frac{1}{C} \\sum W, \\quad \\sigma_W = \\sqrt{\\frac{1}{C} \\sum (W - \\mu_W)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def weight_standardization(W, axis=(1, 2, 3)):\n",
    "    \"\"\"\n",
    "    Normalizes convolutional weights across spatial and feature dimensions.\n",
    "    Used in ResNets and other architectures.\n",
    "    \"\"\"\n",
    "    mean = W.mean(axis=axis, keepdims=True)\n",
    "    std = W.std(axis=axis, keepdims=True)\n",
    "    return (W - mean) / (std + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Normalization  \n",
    "\n",
    "**Spectral Normalization** ensures that the weight matrix has a bounded spectral norm, helping stabilize GAN training.   \n",
    "\n",
    "The spectral norm of a matrix $ W $ is:\n",
    "\n",
    "$$\n",
    "\\| W \\|_{\\sigma} = \\max_{\\| v \\|=1} \\| W v \\|\n",
    "$$\n",
    "\n",
    "Spectral Normalization rescales $ W $ as:\n",
    "\n",
    "$$\n",
    "\\hat{W} = \\frac{W}{\\| W \\|_{\\sigma}}\n",
    "$$\n",
    "\n",
    "where $ \\| W \\|_{\\sigma} $ is estimated using the **power iteration method**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def spectral_norm(W, num_iters=1):\n",
    "    \"\"\"\n",
    "    Normalizes weights using spectral normalization.\n",
    "    Used in stabilizing GAN training.\n",
    "    \"\"\"\n",
    "    u = jax.random.normal(jax.random.PRNGKey(0), (W.shape[1],))  # Initialize u\n",
    "    for _ in range(num_iters):\n",
    "        v = jnp.dot(W.T, u)\n",
    "        v = v / jnp.linalg.norm(v)\n",
    "        u = jnp.dot(W, v)\n",
    "        u = u / jnp.linalg.norm(u)\n",
    "    sigma = jnp.dot(u, jnp.dot(W, v))\n",
    "    return W / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "| Normalization Type  | Normalization Axis | Best Use Cases |\n",
    "|---------------------|--------------------|---------------|\n",
    "| **Batch Normalization** | Over the batch dimension (per feature) | CNNs, large batch sizes |\n",
    "| **Layer Normalization** | Over feature dimensions (per sample) | Transformers, RNNs, NLP |\n",
    "| **Instance Normalization** | Over spatial dimensions (per channel, per sample) | Style transfer, generative models |\n",
    "| **Group Normalization** | Over grouped channels (per sample) | CNNs, small batch sizes |\n",
    "| **Weight Standardization** | On layer weights | ResNets, stable CNN training |\n",
    "| **Spectral Normalization** | Constrains weight spectral norm | GANs, stabilizing adversarial training |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
